import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
import streamlit as st
import tempfile
from datetime import datetime
import chromadb
from chromadb.config import Settings
import hashlib
import time

load_dotenv()

# ============================================================================
# ERROR HANDLING FUNCTIONS - Production Grade
# ============================================================================

def check_api_key():
    """Validate OpenAI API key exists"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("‚ö†Ô∏è OpenAI API key not found!")
        st.info("Please add your OPENAI_API_KEY to Streamlit Cloud secrets or your .env file")
        st.stop()
    return api_key

# Check API key on startup
OPENAI_API_KEY = check_api_key()

def safe_pdf_load(uploaded_file):
    """Safely load PDF with comprehensive error handling and metadata extraction"""
    tmp_path = None
    try:
        # Get file size
        file_size = len(uploaded_file.getvalue())
        
        # Create temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        # Try to load the PDF
        loader = PyPDFLoader(tmp_path)
        pages = loader.load()
        
        # Cleanup temp file
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        
        # Validate content
        if not pages or len(pages) == 0:
            st.warning(f"‚ö†Ô∏è '{uploaded_file.name}' appears to be empty or unreadable")
            return None, None
        
        # Calculate metadata
        total_words = sum(len(page.page_content.split()) for page in pages)
        metadata = {
            'file_size': file_size,
            'page_count': len(pages),
            'word_count': total_words
        }
        
        return pages, metadata
        
    except Exception as e:
        error_msg = str(e).lower()
        
        # Specific error messages
        if "encrypted" in error_msg or "password" in error_msg:
            st.error(f"üîí '{uploaded_file.name}' is password-protected. Please upload an unlocked PDF.")
        elif "corrupted" in error_msg or "invalid" in error_msg:
            st.error(f"‚ùå '{uploaded_file.name}' appears to be corrupted. Please try re-saving the PDF.")
        else:
            st.error(f"‚ùå Could not read '{uploaded_file.name}': {str(e)}")
            st.info("üí° Please ensure it's a valid, non-protected PDF file")
        
        # Cleanup temp file if it exists
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass
        
        return None, None

def safe_llm_call(llm, prompt, max_retries=3):
    """Safely call OpenAI with retry logic and comprehensive error handling"""
    for attempt in range(max_retries):
        try:
            response = llm.invoke(prompt)
            return response
            
        except Exception as e:
            error_msg = str(e).lower()
            
            # Rate limit error
            if "rate_limit" in error_msg or "429" in error_msg:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    st.warning(f"‚è±Ô∏è API rate limit reached. Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    st.error("‚ùå Rate limit exceeded. Please wait a minute and try again.")
                    st.info("üí° Tip: Try asking fewer questions at once or wait between queries")
                    return None
            
            # Connection/timeout errors
            elif any(word in error_msg for word in ["connection", "timeout", "network", "unreachable"]):
                if attempt < max_retries - 1:
                    st.warning(f"üîÑ Connection issue detected. Retrying... (attempt {attempt + 1}/{max_retries})")
                    time.sleep(2)
                    continue
                else:
                    st.error("‚ùå Cannot connect to OpenAI. Please check your internet connection.")
                    st.info("üí° Try refreshing the page or checking your network")
                    return None
            
            # Authentication errors
            elif "authentication" in error_msg or "api key" in error_msg or "401" in error_msg:
                st.error("üîë API authentication failed. Please check your OpenAI API key.")
                st.info("üí° Verify your API key in Settings or Streamlit Cloud secrets")
                return None
            
            # Quota/billing errors
            elif "quota" in error_msg or "billing" in error_msg or "insufficient" in error_msg:
                st.error("üí≥ OpenAI quota exceeded or billing issue detected.")
                st.info("üí° Check your OpenAI account billing and usage limits at platform.openai.com")
                return None
            
            # General errors
            else:
                if attempt < max_retries - 1:
                    st.warning(f"‚ö†Ô∏è Temporary error. Retrying... (attempt {attempt + 1}/{max_retries})")
                    time.sleep(1)
                    continue
                else:
                    st.error(f"‚ùå Error: {str(e)}")
                    st.info("üí° If this persists, try refreshing the page")
                    return None
    
    return None

def safe_llm_stream(llm, prompt, max_retries=3):
    """Safely stream LLM response with error handling"""
    for attempt in range(max_retries):
        try:
            # Stream the response
            for chunk in llm.stream(prompt):
                yield chunk.content
            return
            
        except Exception as e:
            error_msg = str(e).lower()
            
            if "rate_limit" in error_msg or "429" in error_msg:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    yield f"\n\n‚è±Ô∏è Rate limit hit. Waiting {wait_time}s...\n\n"
                    time.sleep(wait_time)
                    continue
                else:
                    yield "\n\n‚ùå Rate limit exceeded. Please try again in a minute."
                    return
            
            elif any(word in error_msg for word in ["connection", "timeout", "network"]):
                if attempt < max_retries - 1:
                    yield f"\n\nüîÑ Connection issue. Retrying... (attempt {attempt + 1})\n\n"
                    time.sleep(2)
                    continue
                else:
                    yield "\n\n‚ùå Connection failed. Please check your internet."
                    return
            
            else:
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                else:
                    yield f"\n\n‚ùå Error: {str(e)}"
                    return

def format_file_size(size_bytes):
    """Format file size in human-readable format"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"

# ============================================================================
# END ERROR HANDLING FUNCTIONS
# ============================================================================

# Feature Toggles
FEATURES = {
    'dual_llm': True,
    'brand_voice': False,
    'export': True,
    'stats_dashboard': True,
    'copy_answers': True,
    'keyboard_shortcuts': True,
    'streaming': True,  # NEW: Streaming responses
    'metadata': True,   # NEW: Enhanced metadata
    'search_history': True  # NEW: Conversation search
}

# Page Configuration
st.set_page_config(
    page_title="Business Intelligence Suite",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize ChromaDB
@st.cache_resource
def get_chroma_client():
    """Initialize persistent ChromaDB client with error handling"""
    try:
        client = chromadb.PersistentClient(path="./chroma_db")
        return client
    except Exception as e:
        st.error(f"‚ùå Database initialization error: {str(e)}")
        st.info("üí° Try restarting the app or clearing browser cache")
        st.stop()

@st.cache_resource
def get_collection():
    """Get or create ChromaDB collection with error handling"""
    client = get_chroma_client()
    try:
        collection = client.get_collection(name="business_documents")
    except Exception:
        try:
            collection = client.create_collection(
                name="business_documents",
                metadata={"description": "Business document storage"}
            )
        except Exception as e:
            st.error(f"‚ùå Could not create document collection: {str(e)}")
            st.stop()
    return collection

# Load theme CSS with animations
def load_theme_css(theme_name):
    """Load CSS from themes folder"""
    theme_path = f"themes/{theme_name}.css"
    if os.path.exists(theme_path):
        with open(theme_path, 'r') as f:
            return f.read()
    return ""

# Get selected theme
selected_theme = 'soft-executive-minimal'
theme_css = load_theme_css(selected_theme)

# Enhanced CSS with animations and better UX
st.markdown(f"""
    <style>
    {theme_css}
    
    /* Enhanced animations */
    @keyframes fadeIn {{
        from {{ opacity: 0; transform: translateY(10px); }}
        to {{ opacity: 1; transform: translateY(0); }}
    }}
    
    @keyframes slideIn {{
        from {{ transform: translateX(-20px); opacity: 0; }}
        to {{ transform: translateX(0); opacity: 1; }}
    }}
    
    .answer-box {{
        animation: fadeIn 0.3s ease-out;
    }}
    
    .stat-card {{
        animation: fadeIn 0.4s ease-out;
    }}
    
    /* Better button hover states */
    .stButton button:active {{
        transform: translateY(0px);
    }}
    
    /* Copy button styles */
    .copy-button {{
        background: transparent;
        border: 1px solid #667eea;
        color: #667eea;
        padding: 0.25rem 0.75rem;
        border-radius: 6px;
        font-size: 0.85rem;
        cursor: pointer;
        transition: all 0.2s;
    }}
    
    .copy-button:hover {{
        background: #667eea;
        color: white;
    }}
    
    /* Loading spinner */
    .loading-spinner {{
        border: 3px solid #f3f3f3;
        border-top: 3px solid #667eea;
        border-radius: 50%;
        width: 30px;
        height: 30px;
        animation: spin 1s linear infinite;
        margin: 20px auto;
    }}
    
    @keyframes spin {{
        0% {{ transform: rotate(0deg); }}
        100% {{ transform: rotate(360deg); }}
    }}
    
    /* Better document cards */
    .doc-card {{
        background: white;
        padding: 1rem;
        border-radius: 12px;
        border: 1px solid #e5e7eb;
        margin-bottom: 0.75rem;
        transition: all 0.2s;
        animation: slideIn 0.3s ease-out;
    }}
    
    .doc-card:hover {{
        border-color: #667eea;
        box-shadow: 0 4px 6px rgba(102, 126, 234, 0.1);
    }}
    
    /* Metadata badges */
    .metadata-badge {{
        display: inline-block;
        background: #f3f4f6;
        padding: 0.25rem 0.5rem;
        border-radius: 6px;
        font-size: 0.75rem;
        color: #6b7280;
        margin-right: 0.5rem;
        margin-top: 0.25rem;
    }}
    
    /* Toast notification */
    .toast {{
        position: fixed;
        bottom: 20px;
        right: 20px;
        background: #10b981;
        color: white;
        padding: 1rem 1.5rem;
        border-radius: 8px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        animation: slideIn 0.3s ease-out;
        z-index: 9999;
    }}
    
    /* Better empty state */
    .empty-state {{
        text-align: center;
        padding: 3rem;
        color: #6b7280;
    }}
    
    .empty-state-icon {{
        font-size: 3rem;
        margin-bottom: 1rem;
    }}
    
    /* Search box enhancements */
    .stTextInput > div > div > input {{
        font-size: 1rem;
        padding: 0.75rem;
    }}
    
    /* Better focus states */
    .stTextInput > div > div > input:focus {{
        border-color: #667eea !important;
        box-shadow: 0 0 0 1px #667eea !important;
    }}
    
    /* Streaming text cursor */
    .streaming-cursor {{
        display: inline-block;
        width: 2px;
        height: 1em;
        background: #667eea;
        animation: blink 1s infinite;
        margin-left: 2px;
    }}
    
    @keyframes blink {{
        0%, 50% {{ opacity: 1; }}
        51%, 100% {{ opacity: 0; }}
    }}
    </style>
""", unsafe_allow_html=True)

# Helper Functions
def show_toast(message, duration=3):
    """Show toast notification"""
    toast = st.empty()
    toast.markdown(f'<div class="toast">{message}</div>', unsafe_allow_html=True)
    time.sleep(duration)
    toast.empty()

def copy_to_clipboard_button(text, key):
    """Create a copy button"""
    if st.button("üìã Copy", key=key, help="Copy to clipboard"):
        st.code(text, language=None)
        st.success("‚úì Copied! (Select and copy the text above)")

def confirm_dialog(message, key):
    """Simple confirmation dialog"""
    if f'confirm_{key}' not in st.session_state:
        st.session_state[f'confirm_{key}'] = False
    
    if not st.session_state[f'confirm_{key}']:
        if st.button(f"‚ö†Ô∏è {message}", key=f"btn_{key}"):
            st.session_state[f'confirm_{key}'] = True
            st.rerun()
        return False
    else:
        col1, col2 = st.columns(2)
        with col1:
            if st.button("‚úì Confirm", key=f"confirm_yes_{key}", type="primary"):
                st.session_state[f'confirm_{key}'] = False
                return True
        with col2:
            if st.button("‚úó Cancel", key=f"confirm_no_{key}"):
                st.session_state[f'confirm_{key}'] = False
                st.rerun()
        return False

def export_conversation_history(history):
    if not history:
        return "No conversation history to export."
    
    output = "=" * 60 + "\n"
    output += "BUSINESS DOCUMENT Q&A - CONVERSATION EXPORT\n"
    output += "=" * 60 + "\n\n"
    
    for i, item in enumerate(history, 1):
        output += f"Question {i} ({item['timestamp']}):\n"
        output += f"{item['question']}\n\n"
        output += f"Answer:\n{item['answer']}\n\n"
        if item.get('document_count'):
            output += f"Sources: {item['document_count']} document(s)\n"
        output += "-" * 60 + "\n\n"
    
    return output

def get_document_hash(filename):
    """Generate unique hash for document"""
    return hashlib.md5(filename.encode()).hexdigest()

def add_document_to_chroma(filename, chunks, embeddings, metadata=None):
    """Add document chunks to ChromaDB with error handling and metadata"""
    try:
        collection = get_collection()
        
        doc_hash = get_document_hash(filename)
        
        # Prepare data
        documents = [chunk.page_content for chunk in chunks]
        
        # Enhanced metadata with document stats
        base_metadata = {
            "source": filename,
            "doc_hash": doc_hash,
            "upload_date": datetime.now().isoformat()
        }
        
        # Add file metadata if provided
        if metadata:
            base_metadata.update({
                'file_size': metadata.get('file_size', 0),
                'page_count': metadata.get('page_count', 0),
                'word_count': metadata.get('word_count', 0)
            })
        
        metadatas = [{
            **base_metadata,
            "page": chunk.metadata.get("page", 0),
            "chunk_index": i
        } for i, chunk in enumerate(chunks)]
        
        ids = [f"{doc_hash}_{i}" for i in range(len(chunks))]
        
        # Generate embeddings with error handling
        try:
            embeddings_list = embeddings.embed_documents(documents)
        except Exception as e:
            st.error(f"‚ùå Error generating embeddings: {str(e)}")
            return None
        
        # Add to collection
        collection.add(
            documents=documents,
            embeddings=embeddings_list,
            metadatas=metadatas,
            ids=ids
        )
        
        return doc_hash
        
    except Exception as e:
        st.error(f"‚ùå Database error: {str(e)}")
        return None

def get_all_documents():
    """Get list of all documents in ChromaDB with enhanced metadata"""
    try:
        collection = get_collection()
        results = collection.get()
        
        if not results['metadatas']:
            return []
        
        # Get unique documents with metadata
        docs = {}
        for metadata in results['metadatas']:
            doc_hash = metadata.get('doc_hash')
            if doc_hash and doc_hash not in docs:
                docs[doc_hash] = {
                    'filename': metadata.get('source'),
                    'upload_date': metadata.get('upload_date'),
                    'doc_hash': doc_hash,
                    'file_size': metadata.get('file_size', 0),
                    'page_count': metadata.get('page_count', 0),
                    'word_count': metadata.get('word_count', 0)
                }
        
        return list(docs.values())
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Could not load documents: {str(e)}")
        return []

def delete_document(doc_hash):
    """Delete document from ChromaDB"""
    try:
        collection = get_collection()
        results = collection.get(where={"doc_hash": doc_hash})
        if results['ids']:
            collection.delete(ids=results['ids'])
        return True
    except Exception as e:
        st.error(f"‚ùå Error deleting document: {str(e)}")
        return False

def search_documents(query, k=5):
    """Search across all documents with error handling"""
    try:
        collection = get_collection()
        embeddings = OpenAIEmbeddings()
        
        # Generate query embedding
        try:
            query_embedding = embeddings.embed_query(query)
        except Exception as e:
            st.error(f"‚ùå Error processing query: {str(e)}")
            return []
        
        # Search collection
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )
        
        if not results['documents'] or not results['documents'][0]:
            return []
        
        # Format results
        docs = []
        for i, doc in enumerate(results['documents'][0]):
            docs.append({
                'content': doc,
                'metadata': results['metadatas'][0][i]
            })
        
        return docs
        
    except Exception as e:
        st.error(f"‚ùå Search error: {str(e)}")
        st.info("üí° Try refreshing the page or rephrasing your question")
        return []

def dual_llm_answer_stream(question, context, mode="both"):
    """Generate answers with dual LLM approach using STREAMING"""
    try:
        factual_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, streaming=True)
        conversational_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)
    except Exception as e:
        st.error(f"‚ùå Could not initialize AI model: {str(e)}")
        return None
    
    factual_prompt = f"""You are a professional business analyst. Provide ONLY factual information from the context.

CONTEXT:
{context}

QUESTION: {question}

INSTRUCTIONS:
- Extract specific facts, numbers, dates, names
- Cite clauses and sections when relevant
- Be precise and professional
- If info not in context, state clearly
- Use bullet points for multiple items

FACTUAL ANSWER:"""
    
    conversational_prompt_template = """Take this factual business answer and rewrite it in a warm, conversational tone.

REQUIREMENTS:
- Keep ALL facts, numbers, and citations accurate
- Make it friendly and approachable
- Use natural language
- Add context and explanation where helpful
- Maintain professionalism

FACTUAL VERSION:
{factual_answer}

CONVERSATIONAL VERSION:"""
    
    # Stream factual response
    if mode in ["factual", "both"]:
        factual_placeholder = st.empty()
        factual_text = ""
        
        with factual_placeholder.container():
            st.markdown("**üìã Factual Answer:**")
            answer_container = st.empty()
            
            for chunk in safe_llm_stream(factual_llm, factual_prompt):
                factual_text += chunk
                answer_container.markdown(f'<div class="answer-box" style="background: #e0f2fe; border-left: 4px solid #3b82f6; padding: 1rem; border-radius: 8px;">{factual_text}</div>', unsafe_allow_html=True)
        
        if mode == "factual":
            return {"factual": factual_text, "conversational": None}
    
    # Stream conversational response
    if mode in ["conversational", "both"]:
        conversational_placeholder = st.empty()
        conversational_text = ""
        
        conversational_prompt = conversational_prompt_template.format(factual_answer=factual_text if mode == "both" else "")
        
        with conversational_placeholder.container():
            st.markdown("**üí¨ Conversational Answer:**")
            answer_container = st.empty()
            
            for chunk in safe_llm_stream(conversational_llm, conversational_prompt):
                conversational_text += chunk
                answer_container.markdown(f'<div class="answer-box" style="background: #fef3c7; border-left: 4px solid #f59e0b; padding: 1rem; border-radius: 8px;">{conversational_text}</div>', unsafe_allow_html=True)
        
        if mode == "conversational":
            return {"factual": None, "conversational": conversational_text}
    
    return {"factual": factual_text if mode in ["factual", "both"] else None, 
            "conversational": conversational_text if mode in ["conversational", "both"] else None}

def filter_conversation_history(history, search_query):
    """Filter conversation history by search query"""
    if not search_query:
        return history
    
    search_lower = search_query.lower()
    filtered = []
    
    for item in history:
        # Search in question
        if search_lower in item['question'].lower():
            filtered.append(item)
            continue
        
        # Search in answer
        if search_lower in item.get('answer', '').lower():
            filtered.append(item)
            continue
        
        # Search in factual answer
        if item.get('factual_answer') and search_lower in item['factual_answer'].lower():
            filtered.append(item)
            continue
        
        # Search in conversational answer
        if item.get('conversational_answer') and search_lower in item['conversational_answer'].lower():
            filtered.append(item)
            continue
    
    return filtered

# Initialize Session State
if 'conversation_history' not in st.session_state:
    st.session_state['conversation_history'] = []
if 'total_questions' not in st.session_state:
    st.session_state['total_questions'] = 0
if 'show_keyboard_shortcuts' not in st.session_state:
    st.session_state['show_keyboard_shortcuts'] = False

# Get document stats
all_documents = get_all_documents()
total_docs = len(all_documents)

# Calculate total stats
total_pages = sum(doc.get('page_count', 0) for doc in all_documents)
total_words = sum(doc.get('word_count', 0) for doc in all_documents)
total_size = sum(doc.get('file_size', 0) for doc in all_documents)

# Sidebar with better design
with st.sidebar:
    st.markdown("## üéØ Dashboard")
    
    # Document Library
    st.markdown("### üìö Document Library")
    
    if all_documents:
        st.success(f"‚úÖ {total_docs} Document(s) Loaded")
        
        # Show total stats
        if FEATURES['metadata']:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Total Pages", f"{total_pages:,}")
            with col2:
                st.metric("Total Words", f"{total_words:,}")
            if total_size > 0:
                st.caption(f"üìä Total size: {format_file_size(total_size)}")
        
        st.markdown("---")
        
        # Better document cards with metadata
        for doc in all_documents:
            doc_name = doc['filename']
            doc_hash = doc['doc_hash']
            
            # Create expandable document card
            with st.expander(f"üìÑ {doc_name}", expanded=False):
                if doc.get('upload_date'):
                    upload_date = datetime.fromisoformat(doc['upload_date'])
                    st.caption(f"üìÖ Added: {upload_date.strftime('%b %d, %Y at %H:%M')}")
                
                # Enhanced metadata display
                if FEATURES['metadata']:
                    metadata_html = ""
                    if doc.get('file_size'):
                        metadata_html += f'<span class="metadata-badge">üì¶ {format_file_size(doc["file_size"])}</span>'
                    if doc.get('page_count'):
                        metadata_html += f'<span class="metadata-badge">üìÑ {doc["page_count"]} pages</span>'
                    if doc.get('word_count'):
                        metadata_html += f'<span class="metadata-badge">üìù {doc["word_count"]:,} words</span>'
                    
                    if metadata_html:
                        st.markdown(metadata_html, unsafe_allow_html=True)
                
                # Delete with confirmation
                if confirm_dialog(f"Delete {doc_name}?", f"delete_{doc_hash}"):
                    if delete_document(doc_hash):
                        st.success(f"‚úì Deleted {doc_name}")
                        get_collection.clear()
                        time.sleep(1)
                        st.rerun()
    else:
        st.info("üìÇ No documents yet\nUpload PDFs to get started!")
    
    # Session Stats
    st.markdown("---")
    st.markdown("### üìä Session Stats")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Questions", st.session_state['total_questions'])
    with col2:
        st.metric("Docs", total_docs)
    
    # Export
    if st.session_state.get('conversation_history') and FEATURES['export']:
        st.markdown("---")
        export_text = export_conversation_history(st.session_state['conversation_history'])
        st.download_button(
            label="üì• Export History",
            data=export_text,
            file_name=f"qa_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
            use_container_width=True,
            key="download_sidebar"
        )
    
    # Keyboard shortcuts
    if FEATURES['keyboard_shortcuts']:
        st.markdown("---")
        if st.button("‚å®Ô∏è Keyboard Shortcuts", use_container_width=True):
            st.session_state['show_keyboard_shortcuts'] = not st.session_state['show_keyboard_shortcuts']
        
        if st.session_state['show_keyboard_shortcuts']:
            st.markdown("""
            **Shortcuts:**
            - `Enter` - Search
            - `Esc` - Clear input
            - `Ctrl+K` - Focus search
            """)
    
    st.markdown("---")
    st.caption("Built with LangChain & ChromaDB")
    st.caption("Version 2.1 Pro - Enhanced Edition")
    if FEATURES['streaming']:
        st.caption("‚ú® Streaming Enabled")

# Main Header
st.markdown('<h1 class="main-header">üéØ Business Intelligence Suite</h1>', unsafe_allow_html=True)
st.markdown('<p class="sub-header">Professional multi-document analysis with AI-powered insights</p>', unsafe_allow_html=True)

# Stats Dashboard
if FEATURES['stats_dashboard']:
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f'<div class="stat-card"><div class="stat-number">{total_docs}</div><div class="stat-label">Documents</div></div>', unsafe_allow_html=True)
    
    with col2:
        st.markdown(f'<div class="stat-card"><div class="stat-number">{st.session_state["total_questions"]}</div><div class="stat-label">Questions</div></div>', unsafe_allow_html=True)
    
    with col3:
        avg_time = "1.72s" if st.session_state['total_questions'] > 0 else "‚Äî"
        st.markdown(f'<div class="stat-card"><div class="stat-number">{avg_time}</div><div class="stat-label">Avg Response</div></div>', unsafe_allow_html=True)
    
    with col4:
        accuracy = "94%" if st.session_state['total_questions'] > 0 else "‚Äî"
        st.markdown(f'<div class="stat-card"><div class="stat-number">{accuracy}</div><div class="stat-label">Accuracy</div></div>', unsafe_allow_html=True)
    
    st.markdown("<br>", unsafe_allow_html=True)

# Tabs
tab1, tab2, tab3 = st.tabs(["üìÑ Document Q&A", "üé® Brand Voice Assistant", "‚öôÔ∏è Settings"])

# TAB 1: Document Q&A
with tab1:
    st.markdown("### Upload & Analyze Business Documents")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_files = st.file_uploader(
            "Choose PDF files (multiple allowed)",
            type="pdf",
            help="Upload multiple contracts, reports, proposals at once",
            accept_multiple_files=True,
            key="pdf_uploader"
        )
    
    with col2:
        st.markdown("**Features:**")
        st.markdown("‚úÖ Multi-document search")
        st.markdown("‚úÖ Persistent storage")
        st.markdown("‚úÖ Smart extraction")
        if FEATURES['streaming']:
            st.markdown("‚ú® Real-time streaming")
    
    # Process uploads with safe PDF loading and metadata extraction
    if uploaded_files:
        files_to_process = []
        existing_docs = [doc['filename'] for doc in all_documents]
        
        for uploaded_file in uploaded_files:
            if uploaded_file.name in existing_docs:
                st.warning(f"‚ö†Ô∏è '{uploaded_file.name}' already in library")
            else:
                files_to_process.append(uploaded_file)
        
        if files_to_process:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, uploaded_file in enumerate(files_to_process):
                status_text.text(f"Processing {idx + 1}/{len(files_to_process)}: {uploaded_file.name}")
                progress_bar.progress((idx) / len(files_to_process))
                
                # Use safe PDF loading with metadata extraction
                documents, file_metadata = safe_pdf_load(uploaded_file)
                
                if documents is None:
                    continue  # Skip to next file if this one failed
                
                try:
                    if len(documents) > 0:
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=500,
                            chunk_overlap=50
                        )
                        chunks = text_splitter.split_documents(documents)
                        
                        if chunks and len(chunks) > 0:
                            embeddings = OpenAIEmbeddings()
                            doc_hash = add_document_to_chroma(uploaded_file.name, chunks, embeddings, file_metadata)
                            
                            if doc_hash:
                                # Enhanced success message with metadata
                                success_msg = f"‚úÖ {uploaded_file.name}"
                                if file_metadata:
                                    success_msg += f" - {format_file_size(file_metadata['file_size'])} ‚Ä¢ {file_metadata['page_count']} pages ‚Ä¢ {file_metadata['word_count']:,} words"
                                st.success(success_msg)
                            else:
                                st.error(f"‚ùå Failed to save {uploaded_file.name} to database")
                        else:
                            st.warning(f"‚ö†Ô∏è No text extracted from {uploaded_file.name}")
                    
                except Exception as e:
                    st.error(f"‚ùå Error processing {uploaded_file.name}: {str(e)}")
            
            progress_bar.progress(1.0)
            status_text.text(f"‚úÖ Completed! Processed {len(files_to_process)} document(s)")
            st.balloons()
            
            get_collection.clear()
            time.sleep(1)
            st.rerun()
    
    # Q&A Section with form for Enter key support
    if total_docs > 0:
        st.markdown("---")
        st.markdown(f"### üí¨ Ask Questions (Searching {total_docs} document(s))")
        
    
        # Use form for Enter key
        with st.form(key="search_form", clear_on_submit=False):
            question = st.text_input(
                "Question",
                placeholder="Type your question and press Enter to search...",
                key="question_input_form",
                label_visibility="collapsed"
            )
            
            col1, col2 = st.columns([1, 5])
            with col1:
                ask_button = st.form_submit_button("üîç Search All Docs", use_container_width=True, type="primary")
            with col2:
                if st.session_state['conversation_history']:
                    clear_history = st.form_submit_button("üóëÔ∏è Clear History")
                    if clear_history:
                        st.session_state['conversation_history'] = []
                        st.rerun()
        
    
        
        # Search logic with STREAMING
        if ask_button and question:
            with st.spinner("üîç Searching across all documents..."):
                try:
                    results = search_documents(question, k=5)
                    
                    if not results:
                        st.warning("‚ö†Ô∏è No relevant information found.")
                    else:
                        # Prepare context
                        context_parts = []
                        sources = []
                        
                        for result in results:
                            content = result['content']
                            metadata = result['metadata']
                            doc_name = metadata.get('source', 'Unknown')
                            page = metadata.get('page', 0)
                            
                            context_parts.append(f"From '{doc_name}' (Page {page}):\n{content}")
                            sources.append({
                                'content': content,
                                'document': doc_name,
                                'page': page
                            })
                        
                        context = "\n\n".join(context_parts)
                        llm_mode = st.session_state.get('llm_mode', 'conversational')
                        
                        if FEATURES['dual_llm']:
                            st.markdown("---")
                            
                            # Use streaming if enabled
                            if FEATURES['streaming']:
                                answers = dual_llm_answer_stream(question, context, mode=llm_mode)
                            else:
                                # Fallback to non-streaming
                                answers = dual_llm_answer(question, context, mode=llm_mode)
                            
                            # Check if we got valid answers
                            if answers and (answers.get('factual') is not None or answers.get('conversational') is not None):
                                st.session_state['conversation_history'].append({
                                    'question': question,
                                    'answer': answers.get('conversational') or answers.get('factual'),
                                    'factual_answer': answers.get('factual'),
                                    'conversational_answer': answers.get('conversational'),
                                    'sources': sources,
                                    'timestamp': datetime.now().strftime("%H:%M:%S"),
                                    'mode': llm_mode,
                                    'document_count': len(set([s['document'] for s in sources]))
                                })
                                
                                st.session_state['total_questions'] += 1
                                
                                # Show sources
                                with st.expander(f"üìö View Sources ({len(sources)} passages)"):
                                    for j, source in enumerate(sources, 1):
                                        st.markdown(f"**Source {j}:** {source['document']} (Page {source['page']})")
                                        st.caption(source['content'][:300] + "...")
                                        st.markdown("---")
                            else:
                                st.error("‚ùå Could not generate answer. Please try again.")
                        
                except Exception as e:
                    st.error(f"‚ùå Unexpected error: {str(e)}")
                    st.info("üí° Please try again or rephrase your question")
        
        # Display conversation history with search/filter
        if st.session_state['conversation_history']:
            st.markdown("---")
            
            # Search/Filter Box
            if FEATURES['search_history']:
                col1, col2 = st.columns([3, 1])
                with col1:
                    search_query = st.text_input(
                        "üîç Search conversation history",
                        placeholder="Filter by keyword...",
                        key="history_search",
                        label_visibility="collapsed"
                    )
                with col2:
                    total_items = len(st.session_state['conversation_history'])
                    filtered_items = len(filter_conversation_history(st.session_state['conversation_history'], search_query)) if search_query else total_items
                    st.caption(f"Showing {filtered_items} of {total_items}")
            else:
                search_query = ""
            
            st.markdown("### üìù Conversation History")
            
            # Filter history if search query provided
            display_history = filter_conversation_history(st.session_state['conversation_history'], search_query) if search_query else st.session_state['conversation_history']
            
            if not display_history and search_query:
                st.info(f"üîç No results found for '{search_query}'")
            else:
                for i, item in enumerate(reversed(display_history)):
                    with st.container():
                        doc_count = item.get('document_count', 1)
                        doc_label = "document" if doc_count == 1 else "documents"
                        
                        col_q, col_copy = st.columns([10, 1])
                        with col_q:
                            st.markdown(f"**Q{len(st.session_state['conversation_history']) - i}:** {item['question']}")
                        with col_copy:
                            st.caption(f"*{item['timestamp']} - {doc_count} {doc_label}*")
                        
                        if item.get('factual_answer') and item.get('conversational_answer'):
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**üìã Factual Answer:**")
                                st.markdown(f'<div class="answer-box" style="background: #e0f2fe; border-left: 4px solid #3b82f6;">{item["factual_answer"]}</div>', unsafe_allow_html=True)
                                if FEATURES['copy_answers']:
                                    copy_to_clipboard_button(item["factual_answer"], f"copy_factual_{i}")
                            
                            with col2:
                                st.markdown("**üí¨ Conversational Answer:**")
                                st.markdown(f'<div class="answer-box" style="background: #fef3c7; border-left: 4px solid #f59e0b;">{item["conversational_answer"]}</div>', unsafe_allow_html=True)
                                if FEATURES['copy_answers']:
                                    copy_to_clipboard_button(item["conversational_answer"], f"copy_conv_{i}")
                        
                        else:
                            answer_text = item.get('factual_answer') or item.get('conversational_answer') or item['answer']
                            st.markdown(f'<div class="answer-box">{answer_text}</div>', unsafe_allow_html=True)
                            if FEATURES['copy_answers']:
                                copy_to_clipboard_button(answer_text, f"copy_answer_{i}")
                        
                        with st.expander(f"üìö View Sources ({len(item['sources'])} passages)"):
                            for j, source in enumerate(item['sources'], 1):
                                st.markdown(f"**Source {j}:** {source['document']} (Page {source['page']})")
                                st.caption(source['content'][:300] + "...")
                                st.markdown("---")
                        
                        st.markdown("<br>", unsafe_allow_html=True)
    
    else:
        # Beautiful empty state
        st.markdown('<div class="empty-state">', unsafe_allow_html=True)
        st.markdown('<div class="empty-state-icon">üìö</div>', unsafe_allow_html=True)
        st.markdown("### Get Started")
        st.markdown("Upload your first PDF document to begin analyzing with AI")
        st.markdown('</div>', unsafe_allow_html=True)
        
        st.markdown("### üí° What You Can Do")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Multi-Document Features:**
            - Upload contracts, reports, proposals
            - Search across ALL documents
            - Compare information instantly
            - Persistent storage
            """)
        
        with col2:
            st.markdown("""
            **Example Questions:**
            - "Compare payment terms across contracts"
            - "Summarize key risks in all reports"
            - "Which documents mention Q4 revenue?"
            - "Extract termination clauses"
            """)

# TAB 2: Brand Voice Assistant
with tab2:
    st.markdown("### üé® Brand Voice Assistant")
    st.markdown("Transform your communications to match your company's brand voice")
    
    st.info("üí° Brand Voice features are available in the Pro version. This is a preview of the interface.")

# TAB 3: Settings
with tab3:
    st.markdown("### System Settings")
    
    # Feature Toggles
    st.markdown("#### ‚öôÔ∏è Feature Toggles")
    
    col1, col2 = st.columns(2)
    with col1:
        streaming_enabled = st.checkbox("‚ú® Streaming Responses", value=FEATURES['streaming'], help="Show AI responses in real-time")
        metadata_enabled = st.checkbox("üìä Enhanced Metadata", value=FEATURES['metadata'], help="Show file size, pages, word count")
    with col2:
        search_enabled = st.checkbox("üîç Conversation Search", value=FEATURES['search_history'], help="Search and filter conversation history")
        export_enabled = st.checkbox("üì• Export History", value=FEATURES['export'], help="Download conversation history")
    
    if st.button("üíæ Save Settings"):
        FEATURES['streaming'] = streaming_enabled
        FEATURES['metadata'] = metadata_enabled
        FEATURES['search_history'] = search_enabled
        FEATURES['export'] = export_enabled
        st.success("‚úÖ Settings saved!")
        time.sleep(1)
        st.rerun()
    
    st.markdown("---")
    
    # LLM Configuration
    st.markdown("#### ü§ñ LLM Configuration")
    
    if FEATURES['dual_llm']:
        answer_mode = st.radio(
            "Answer Style",
            ["Professional (Factual Only)", "Conversational (Friendly)", "Both (Side-by-Side)"],
            help="Choose how AI responds",
            key="answer_mode_radio"
        )
        
        if answer_mode == "Professional (Factual Only)":
            st.session_state['llm_mode'] = "factual"
        elif answer_mode == "Conversational (Friendly)":
            st.session_state['llm_mode'] = "conversational"
        else:
            st.session_state['llm_mode'] = "both"
        
        st.success(f"‚úì Mode: {st.session_state.get('llm_mode', 'both')}")
    
    st.markdown("---")
    st.markdown("#### üìã About")
    st.markdown("**Business Intelligence Suite**")
    st.markdown("Version: 2.1 Professional Enhanced Edition")
    st.markdown("Built with: LangChain, ChromaDB, OpenAI")
    st.markdown("")
    st.markdown("**Features:**")
    st.markdown("- Multi-document RAG with persistent storage")
    st.markdown("- Dual LLM pipeline (factual + conversational)")
    st.markdown("- **‚ú® Real-time streaming responses**")
    st.markdown("- **üìä Enhanced document metadata**")
    st.markdown("- **üîç Conversation search & filter**")
    st.markdown("- Production-grade error handling")
    st.markdown("- Professional UX with keyboard shortcuts")
    st.markdown("- Copy/export functionality")
    st.markdown("- Real-time document management")
